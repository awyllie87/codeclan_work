---
title: "Decision Trees"
output: html_notebook
---

```{r}
library(rpart)
library(rpart.plot)
library(dplyr)
library(tidyverse)
```

```{r}
thrones <- read_csv("data/character_data_S01-S08.csv")
```

## Data cleaning

Don't need to scale the data due to the Boolean operations at the nodes

We do still need to remove NAs and do some necessary variable reduction

Some variable engineering required to ensure all columns are numeric or factor

We are going to use our dataset to build a decision tree predicting whether a character will die by the end of the series:
- can remove columns directly connected to death
- can remove totally irrelevant columns such as name

Then we'll replace numerics with strings using the data dictionary so that the factor levels have an easily interpretable meaning.

e.g. turn religion = 6 into horse religion

```{r}
clean_thrones<- thrones %>% 
  # Only keep variables of interest
  select(c(sex, religion, occupation, social_status, allegiance_last, allegiance_switched, dth_flag, featured_episode_count, prominence)) %>% 
  # Convert to factor level
  mutate(sex = factor(sex, levels = c(1, 2, 9), labels = c("Male", "Female", "Unknown")),
         religion = factor(religion, levels = c(0, 1, 3, 4, 5, 6, 7, 9), labels = c("Great Stallion", "Lord of Light", "Faith of the Seven", "Old Gods", "Drowned God", "Many Faced God", "Other", "Unknown")),
         occupation = factor(occupation, levels = c(1, 2, 9), labels = c("Silk-collar", "Leather-collar", "Unknown")),
         social_status = factor(social_status, levels = c(1, 2, 9), labels = c("Highborn", "Lowborn", "Unknown")),
         allegiance_last = factor(allegiance_last, levels = c(1, 2, 3, 4, 5, 6, 7, 8, 9), labels = c("Stark", "Targaryen", "Night's Watch", "Lannister", "Greyjoy", "Bolton", "Frey", "Other", "Unknown")),
         allegiance_switched = factor(allegiance_switched, levels = c(1, 2, 9), labels = c("No", "Yes", "Unknown")),
         dth_flag = factor(dth_flag, levels = c(0, 1), labels = c("Survived", "Died"))) %>%
  # Remove NAs 
  na.omit()

glimpse(clean_thrones)
```

We can see that we have 345 outcomes with 9 variables

## Train/test split

Next, we need to create training and testing data sets

80% training, 20% testing.

Let's partition the data:

```{r}
# get how many rows we have in total to work out the percentage
n_data <- nrow(clean_thrones)

# create a test sample index
test_index <- sample(1:n_data, size = n_data*0.2)

# create a test set
thrones_test <- slice(clean_thrones, test_index)

# create a training set
thrones_train <- slice(clean_thrones, -test_index)
```

We can check that our test and training sets have similar proportions of deceased characters.

```{r}
thrones_test %>% 
  tabyl(dth_flag)
```

```{r}
thrones_train %>% 
  tabyl(dth_flag)
```

Seems ok to me!

## Build a tree model based on training data

Going to use the `rpart` library - stands for recursive partitioning and regression trees

- pass it the target variable (deaths, so `dth_flag`) and the dataset to use (`thrones_train`)
- then pass it the variable we are looking for
    `class` for categorical and `anova` for continuous

```{r}
thrones_fit <- rpart(
  formula = dth_flag ~ .,
  data = thrones_train,
  method = "class"
)

rpart.plot(thrones_fit,
           yesno = 2, # 0 = don't write yes/no, 1: write y/n at the top, 2: write y/n at all splits
           fallen.leaves = TRUE,
           faclen = 10,
           digits = 4,
           split.border.col = 1)
```

Variables picked out are the ones deemed most informative for predicting - the rest are discarded

Each node tells us:
  - predicted survived or died is shown on top line (root node shows us all-data-together prediction)
  - prob of a died result is on the second line
  - % of data points that pass through this node on third line
  - colour related to classification and strength of probability
  
We can also plot with counts instead of probabilities:

```{r}
rpart.plot(thrones_fit,
           yesno = 2,
           fallen.leaves = TRUE,
           faclen = 10,
           digits = 4,
           type = 4, # label all nodes, not just leaves
           extra = 101) # displays the number and % of observations in node)
```

We can see the rules used to make the tree if we type:

```{r}
rpart.rules(thrones_fit, cover = TRUE)
```

Note: %s are rounded to the neatest whole number - so sometimes won't make a perfect 100%

## Use trained model to create predictions on the test dataset

```{r}
library(modelr)

## add the predictions

thrones_test_pred <- thrones_test %>% 
  add_predictions(thrones_fit, type = "class")
```

```{r}
# let's look at our prediction, using the most informative variables:
thrones_test_pred %>% 
  select(prominence, religion, allegiance_last, featured_episode_count, dth_flag, pred)
```

## Checking model performance

We can create a confusion matrix again to check performance.

Values read from the left are predicted, values from the top down are actual values
- we tell `conf_mat()` which variable is the true value and predicted value using the arguments:
  `truth` and `estimate`
  
```{r}
library(yardstick)

conf_mat <- thrones_test_pred %>% 
  conf_mat(truth = dth_flag, estimate = pred)

conf_mat
```
Accurate decision tree = higher main diagonal values (top left - bottom right)

Calculate probability of prediction being correct:

```{r}
accuracy <- thrones_test_pred %>% 
  accuracy(truth = dth_flag, estimate = pred)

accuracy
```

the `.estimate` column in the output shows you the probability of correctly predicting whether a character in the test set died or not in the series.

We can also calculate the **sensitivity** (AKA the true positive rate)
and the **specificity** (AKA true negative rate) using other `yardstick` functions

```{r}
thrones_test_pred %>% 
  sensitivity(truth = dth_flag, estimate = pred)

thrones_test_pred %>% 
  specificity(truth = dth_flag, estimate = pred)
```

```{r}
library(caret)

prediction = predict(thrones_fit, newdata = thrones_test, type = 'class')
confusionMatrix(prediction, thrones_test$dth_flag)
```

Congrats on creating a decision tree!

**Advantages**:
  - intuitively easy to explain
  - closely mirror human decision making compared to e.g. regression
  - graphical display
  - easily handle qualitative predictors with the need for dummy variables
  - not sensitive to scale of variables
  
**Disadvantages**
  - trees generally do not have the same level of predictive accuracy as other approaches
    - since they're not quite as robust
    - a small change in the data can cause a large change in th final estimated tree
    
## Random forests

To make this method more robust, the **random forest** algorithm has been developed.

This involes a collection of decision trees (generated by bootstrapping your training data)

Instead of searching for the most important feature while splitting a node, it searches for the best feature among a random subset of features.

A forest: a collection of decision trees that have been built which each have their own class prediction
  - the class with the most votes becomes our model prediction.
  
e.g. if 50 of your trees predict an outcome as true and 10 predict false, the overall prediction of the forest will be true

```{r}
# we can create a random forest classifier using the `ranger` package

library(ranger)

rf_classifier <- ranger(dth_flag ~ .,
                        data = thrones_train,
                        importance = "impurity",
                        num.trees = 1000,
                        mtry = 2, # number of variables to possibly split at in each node
                        min.node.size = 5)

rf_classifier
```

*Hyperparameters*

train on training set 80/100
test on testing set   20/100

adding validation, split this something like 60/20/20

train on training set           60/100
test on testing set             20/100
validation on validation set    20/20

When we're happy that the model performs well enough to the validation dataset

combine the val data and train

test again

k-fold cv includes the validation process, so you might be fine just validating that way.

```{r}
rf_classifier
```
```{r}
sort(importance(rf_classifier), decreasing = TRUE)
```

```{r}
confusionMatrix(thrones_test_pred$pred, thrones_test_pred$dth_flag)
```

